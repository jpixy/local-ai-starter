# Local AI Starter with Ollama and Qwen Models

ä¸€ä¸ªå®Œæ•´çš„æœ¬åœ° AI è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨ Ollama å’Œ Qwen ç³»åˆ—æ¨¡å‹ï¼ˆ7B/32B/72Bï¼‰ï¼Œæ”¯æŒ Docker Compose ä¸€é”®éƒ¨ç½²å’Œ Python å®¢æˆ·ç«¯é›†æˆã€‚

## âš¡ å¿«é€Ÿå‚è€ƒ

```bash
# ğŸš€ å¯åŠ¨æœåŠ¡ï¼ˆDocker Composeï¼‰
docker compose -f docker-compose-complete.yml up -d

# ğŸ“Š æŸ¥çœ‹çŠ¶æ€
docker compose -f docker-compose-complete.yml ps

# ğŸ” æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
docker exec ollama-service ollama list

# ğŸ§ª å¿«é€Ÿæµ‹è¯•
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:7b", "prompt": "ä½ å¥½", "stream": false}'

# ğŸ Python æµ‹è¯•
poetry run python test_client.py

# ğŸ“ æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose-complete.yml logs -f ollama

# ğŸ›‘ åœæ­¢æœåŠ¡
docker compose -f docker-compose-complete.yml down
```

**å½“å‰æœåŠ¡å™¨è®¿é—®åœ°å€ï¼š**
- æœ¬åœ°ï¼š`http://localhost:11434`
- å¤–éƒ¨ï¼š`http://10.176.202.207:11434`ï¼ˆæ ¹æ®å®é™… IPï¼‰

## âœ¨ ç‰¹æ€§

- **ğŸ³ Docker éƒ¨ç½²**: ä½¿ç”¨ Docker Compose ä¸€é”®éƒ¨ç½²å®Œæ•´æœåŠ¡æ ˆï¼ˆOllama + Nginxï¼‰
- **ğŸš€ å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Qwen 2.5 7B/32B/72B æ¨¡å‹ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©
- **ğŸ”Œ Python é›†æˆ**: æä¾›åŒæ­¥å’Œå¼‚æ­¥ Python å®¢æˆ·ç«¯åº“
- **ğŸŒ å¤–éƒ¨è®¿é—®**: é…ç½® Nginx åå‘ä»£ç†ï¼Œæ”¯æŒåŸŸåå’Œå¤–éƒ¨è®¿é—®
- **âš¡ GPU åŠ é€Ÿ**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ NVIDIA GPU åŠ é€Ÿæ¨ç†
- **ğŸ“Š å¥åº·æ£€æŸ¥**: å®Œæ•´çš„æœåŠ¡å¥åº·æ£€æŸ¥å’Œç›‘æ§
- **ğŸ§ª å®Œæ•´æµ‹è¯•**: æä¾›å…¨é¢çš„æµ‹è¯•å¥—ä»¶å’Œç¤ºä¾‹

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚ï¼ˆæ ¹æ®æ¨¡å‹é€‰æ‹©ï¼‰

| æ¨¡å‹ | æœ€å°å†…å­˜ | æ¨èå†…å­˜ | ç£ç›˜ç©ºé—´ | GPU æ˜¾å­˜ |
|------|---------|---------|---------|---------|
| Qwen 2.5 7B | 8GB | 16GB | 5GB | 6GB+ |
| Qwen 2.5 32B | 16GB | 32GB | 20GB | 24GB+ |
| Qwen 2.5 72B | 32GB | 64GB | 50GB | 48GB+ |

### è½¯ä»¶è¦æ±‚

- Linux ç³»ç»Ÿï¼ˆæ¨è Ubuntu 20.04+ï¼‰
- Docker å’Œ Docker Compose
- Python 3.9+ï¼ˆç”¨äºå®¢æˆ·ç«¯ï¼‰
- NVIDIA GPU + nvidia-dockerï¼ˆå¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿï¼‰
- ç¨³å®šçš„ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡ä¸‹è½½æ¨¡å‹ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šDocker Compose éƒ¨ç½²ï¼ˆæ¨èï¼‰

è¿™æ˜¯æœ€ç®€å•ã€æœ€ç¨³å®šçš„éƒ¨ç½²æ–¹å¼ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚

#### 1. æ£€æŸ¥ Docker ç¯å¢ƒ

```bash
# ç¡®ä¿ Docker å’Œ Docker Compose å·²å®‰è£…
docker --version
docker compose version

# å¦‚æœæœ‰ NVIDIA GPUï¼Œæ£€æŸ¥ nvidia-docker
nvidia-smi
```

#### 2. å¯åŠ¨æœåŠ¡

```bash
# ä¸€é”®å¯åŠ¨å®Œæ•´æœåŠ¡æ ˆï¼ˆOllama + Nginxï¼‰
./start-complete.sh

# æˆ–è€…æ‰‹åŠ¨å¯åŠ¨
docker compose -f docker-compose-complete.yml up -d
```

**é¦–æ¬¡å¯åŠ¨è¯´æ˜ï¼š**
- ä¼šè‡ªåŠ¨ä¸‹è½½ Ollama é•œåƒ
- ä¼šè‡ªåŠ¨æ‹‰å– Qwen 2.5 7B æ¨¡å‹ï¼ˆçº¦ 4.7GBï¼‰
- å¦‚æœé…ç½®äº† 32B/72B æ¨¡å‹ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
- é¢„è®¡é¦–æ¬¡å¯åŠ¨æ—¶é—´ï¼š10-30 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰

#### 3. æ£€æŸ¥æœåŠ¡çŠ¶æ€

```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker compose -f docker-compose-complete.yml ps

# æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose-complete.yml logs -f ollama

# æ£€æŸ¥å·²å®‰è£…çš„æ¨¡å‹
docker exec ollama-service ollama list
```

#### 4. æµ‹è¯•æœåŠ¡

```bash
# è·å–æœåŠ¡å™¨ IP
SERVER_IP=$(hostname -I | awk '{print $1}')

# æµ‹è¯• API è¿æ¥
curl http://localhost:11434/api/tags

# æµ‹è¯•æ¨¡å‹ç”Ÿæˆï¼ˆä½¿ç”¨ 7B æ¨¡å‹ï¼Œå“åº”å¿«ï¼‰
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
    "stream": false
  }'
```

#### 5. é…ç½® Python å®¢æˆ·ç«¯

```bash
# å®‰è£… Python ä¾èµ–ï¼ˆä½¿ç”¨ Poetryï¼‰
poetry install

# æˆ–ä½¿ç”¨ pip
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•è„šæœ¬
poetry run python test_client.py
```

### æ–¹å¼äºŒï¼šç›´æ¥åœ¨ä¸»æœºä¸Šè¿è¡Œ Ollama

å¦‚æœä¸æƒ³ä½¿ç”¨ Dockerï¼Œå¯ä»¥ç›´æ¥åœ¨ä¸»æœºä¸Šå®‰è£… Ollamaã€‚

#### 1. å®‰è£… Ollama

```bash
# ä½¿ç”¨å®˜æ–¹å®‰è£…è„šæœ¬
curl -fsSL https://ollama.ai/install.sh | sh

# æˆ–ä½¿ç”¨é¡¹ç›®æä¾›çš„è„šæœ¬
chmod +x setup_ollama.sh
./setup_ollama.sh
```

#### 2. é…ç½®å¤–éƒ¨è®¿é—®

```bash
# é…ç½® Ollama ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/override.conf << EOF
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
EOF

# é‡å¯æœåŠ¡
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

#### 3. ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½ 7B æ¨¡å‹ï¼ˆæ¨èï¼Œå“åº”å¿«ï¼‰
ollama pull qwen2.5:7b

# ä¸‹è½½ 32B æ¨¡å‹ï¼ˆå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰
ollama pull qwen2.5:32b

# ä¸‹è½½ 72B æ¨¡å‹ï¼ˆæœ€ä½³è´¨é‡ï¼Œéœ€è¦å¤§é‡èµ„æºï¼‰
ollama pull qwen2.5:72b

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list
```

#### 4. æµ‹è¯•æœåŠ¡

```bash
# å‘½ä»¤è¡Œäº¤äº’æµ‹è¯•
ollama run qwen2.5:7b

# API æµ‹è¯•
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:7b", "prompt": "Hello", "stream": false}'
```

## ğŸ¯ æ¨¡å‹é€‰æ‹©æŒ‡å—

æ ¹æ®æ‚¨çš„ç¡¬ä»¶èµ„æºå’Œä½¿ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

### Qwen 2.5 7Bï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- æ—¥å¸¸å¯¹è¯å’Œé—®ç­”
- ä»£ç ç”Ÿæˆå’Œè§£é‡Š
- æ–‡æœ¬æ‘˜è¦å’Œç¿»è¯‘
- å¿«é€ŸåŸå‹å¼€å‘

**æ€§èƒ½ç‰¹ç‚¹ï¼š**
- å“åº”é€Ÿåº¦å¿«ï¼ˆ2-5ç§’ï¼‰
- å†…å­˜å ç”¨ä½ï¼ˆ8GB å¯è¿è¡Œï¼‰
- é€‚åˆç¬”è®°æœ¬å’Œå·¥ä½œç«™

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
# Docker ç¯å¢ƒ
docker exec ollama-service ollama run qwen2.5:7b

# ä¸»æœºç¯å¢ƒ
ollama run qwen2.5:7b

# API è°ƒç”¨
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:7b", "prompt": "ä½ çš„é—®é¢˜"}'
```

### Qwen 2.5 32Bï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤æ‚æ¨ç†ä»»åŠ¡
- ä¸“ä¸šé¢†åŸŸé—®ç­”
- é•¿æ–‡æœ¬å¤„ç†
- ä»£ç å®¡æŸ¥å’Œä¼˜åŒ–

**æ€§èƒ½ç‰¹ç‚¹ï¼š**
- å“åº”é€Ÿåº¦ä¸­ç­‰ï¼ˆ5-15ç§’ï¼‰
- å†…å­˜å ç”¨ä¸­ç­‰ï¼ˆ16-32GBï¼‰
- è´¨é‡æ˜æ˜¾ä¼˜äº 7B

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
# åˆ‡æ¢åˆ° 32B æ¨¡å‹
docker exec ollama-service ollama run qwen2.5:32b

# API è°ƒç”¨
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:32b", "prompt": "ä½ çš„é—®é¢˜"}'
```

### Qwen 2.5 72Bï¼ˆæœ€ä½³è´¨é‡ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- é«˜è´¨é‡å†…å®¹åˆ›ä½œ
- å¤æ‚é€»è¾‘æ¨ç†
- ä¸“ä¸šæŠ€æœ¯å’¨è¯¢
- ç ”ç©¶å’Œåˆ†æ

**æ€§èƒ½ç‰¹ç‚¹ï¼š**
- å“åº”é€Ÿåº¦è¾ƒæ…¢ï¼ˆ15-60ç§’ï¼‰
- å†…å­˜å ç”¨é«˜ï¼ˆ32-64GBï¼‰
- è´¨é‡æ¥è¿‘ GPT-4

**ä½¿ç”¨æ–¹æ³•ï¼š**
```bash
# åˆ‡æ¢åˆ° 72B æ¨¡å‹
docker exec ollama-service ollama run qwen2.5:72b

# API è°ƒç”¨
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:72b", "prompt": "ä½ çš„é—®é¢˜"}'
```

## ğŸ’» Python å®¢æˆ·ç«¯ä½¿ç”¨

### åŸºç¡€ç¤ºä¾‹

```python
from ollama_client import create_client

# åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆé»˜è®¤ä½¿ç”¨ qwen2.5:7bï¼‰
client = create_client()

# å¥åº·æ£€æŸ¥
if client.health_check():
    # ç”Ÿæˆæ–‡æœ¬
    response = client.generate("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    print(response['response'])
```

### æŒ‡å®šæ¨¡å‹

```python
from ollama_client import OllamaConfig, OllamaClient

# ä½¿ç”¨ 32B æ¨¡å‹
config = OllamaConfig(
    host="localhost",
    port=11434,
    model="qwen2.5:32b",  # æŒ‡å®šæ¨¡å‹
    timeout=60
)
client = OllamaClient(config)

response = client.generate("è§£é‡Šé‡å­è®¡ç®—çš„åŸç†")
print(response['response'])
```

### èŠå¤©å¯¹è¯

```python
# å¤šè½®å¯¹è¯
messages = [
    {"role": "user", "content": "ä½ å¥½ï¼èƒ½å¸®æˆ‘å­¦ä¹  Python å—ï¼Ÿ"}
]

response = client.chat(messages)
print(response['message']['content'])

# ç»§ç»­å¯¹è¯
messages.append(response['message'])
messages.append({"role": "user", "content": "å¦‚ä½•å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Ÿ"})
response = client.chat(messages)
print(response['message']['content'])
```

### æµå¼å“åº”

```python
# å®æ—¶æµå¼è¾“å‡º
print("AI å›ç­”ï¼š", end='')
for chunk in client.generate_stream("è®²ä¸€ä¸ªå…³äºAIçš„æ•…äº‹"):
    if chunk.get('response'):
        print(chunk['response'], end='', flush=True)
    if chunk.get('done'):
        break
print()
```

### å¼‚æ­¥ä½¿ç”¨

```python
import asyncio
from ollama_client import create_async_client

async def async_example():
    async with create_async_client() as client:
        # å¼‚æ­¥ç”Ÿæˆ
        result = await client.generate("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
        print(result['response'])
        
        # å¼‚æ­¥æµå¼
        async for chunk in client.generate_stream("è§£é‡Šç¥ç»ç½‘ç»œ"):
            if chunk.get('response'):
                print(chunk['response'], end='', flush=True)
            if chunk.get('done'):
                break

asyncio.run(async_example())
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨è¯»å–ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# åˆ›å»º .env æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
cat > .env << EOF
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_TIMEOUT=60
EOF
```

### è‡ªå®šä¹‰é…ç½®

```python
from ollama_client import OllamaConfig, OllamaClient

# è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨
config = OllamaConfig(
    host="10.176.202.207",  # è¿œç¨‹æœåŠ¡å™¨ IP
    port=11434,
    model="qwen2.5:32b",
    timeout=120  # å¤§æ¨¡å‹éœ€è¦æ›´é•¿è¶…æ—¶æ—¶é—´
)

client = OllamaClient(config)
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### å¿«é€Ÿæµ‹è¯•

è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
# ä½¿ç”¨ Poetryï¼ˆæ¨èï¼‰
poetry run python test_client.py

# æˆ–ä½¿ç”¨ Makefile
make test

# æˆ–ç›´æ¥è¿è¡Œ
python test_client.py
```

### æµ‹è¯•å†…å®¹

æµ‹è¯•è„šæœ¬ä¼šéªŒè¯ä»¥ä¸‹åŠŸèƒ½ï¼š
- âœ… æœåŠ¡è¿æ¥æ€§æ£€æŸ¥
- âœ… æ¨¡å‹åˆ—è¡¨è·å–
- âœ… æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
- âœ… èŠå¤©å¯¹è¯æ¥å£
- âœ… æµå¼å“åº”
- âœ… å¼‚æ­¥åŠŸèƒ½

### æ‰‹åŠ¨æµ‹è¯•å‘½ä»¤

```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags

# 2. æµ‹è¯• 7B æ¨¡å‹ï¼ˆå¿«é€Ÿï¼‰
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½",
    "stream": false
  }' | python3 -m json.tool

# 3. æµ‹è¯• 32B æ¨¡å‹ï¼ˆè´¨é‡æ›´å¥½ï¼‰
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5:32b",
    "prompt": "è§£é‡Šæ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†",
    "stream": false
  }' | python3 -m json.tool

# 4. æµ‹è¯•èŠå¤©æ¥å£
curl http://localhost:11434/api/chat -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5:7b",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]
  }' | python3 -m json.tool

# 5. æµ‹è¯•æµå¼å“åº”
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "è®²ä¸€ä¸ªç¬‘è¯",
    "stream": true
  }'
```

## ğŸ”§ æœåŠ¡ç®¡ç†

### Docker Compose å‘½ä»¤

```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker compose -f docker-compose-complete.yml ps

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
docker compose -f docker-compose-complete.yml logs -f

# æŸ¥çœ‹ Ollama æœåŠ¡æ—¥å¿—
docker compose -f docker-compose-complete.yml logs -f ollama

# æŸ¥çœ‹ Nginx æ—¥å¿—
docker compose -f docker-compose-complete.yml logs -f nginx

# é‡å¯æœåŠ¡
docker compose -f docker-compose-complete.yml restart

# é‡å¯ç‰¹å®šæœåŠ¡
docker compose -f docker-compose-complete.yml restart ollama

# åœæ­¢æœåŠ¡
docker compose -f docker-compose-complete.yml down

# åœæ­¢å¹¶åˆ é™¤æ•°æ®å·ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
docker compose -f docker-compose-complete.yml down -v

# å¯åŠ¨æœåŠ¡
docker compose -f docker-compose-complete.yml up -d

# æŸ¥çœ‹èµ„æºå ç”¨
docker stats ollama-service nginx-proxy
```

### å®¹å™¨å†…æ“ä½œ

```bash
# è¿›å…¥ Ollama å®¹å™¨
docker exec -it ollama-service bash

# åœ¨å®¹å™¨å†…æŸ¥çœ‹æ¨¡å‹
docker exec ollama-service ollama list

# åœ¨å®¹å™¨å†…è¿è¡Œæ¨¡å‹
docker exec -it ollama-service ollama run qwen2.5:7b

# åœ¨å®¹å™¨å†…ä¸‹è½½æ–°æ¨¡å‹
docker exec ollama-service ollama pull qwen2.5:14b

# åœ¨å®¹å™¨å†…åˆ é™¤æ¨¡å‹
docker exec ollama-service ollama rm qwen2.5:72b
```

### æ¨¡å‹ç®¡ç†

```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
docker exec ollama-service ollama list

# æŸ¥çœ‹æ¨¡å‹è¯¦ç»†ä¿¡æ¯
docker exec ollama-service ollama show qwen2.5:7b

# ä¸‹è½½å…¶ä»–æ¨¡å‹
docker exec ollama-service ollama pull llama2:7b
docker exec ollama-service ollama pull codellama:13b
docker exec ollama-service ollama pull mistral:7b

# åˆ é™¤ä¸éœ€è¦çš„æ¨¡å‹ï¼ˆé‡Šæ”¾ç©ºé—´ï¼‰
docker exec ollama-service ollama rm qwen2.5:72b
```

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### ä½¿ç”¨ Poetryï¼ˆæ¨èï¼‰

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
poetry shell

# å®‰è£…ä¾èµ–
poetry install

# å®‰è£…å®Œæ•´ä¾èµ–ï¼ˆåŒ…æ‹¬å¯é€‰åŠŸèƒ½ï¼‰
poetry install --extras "full"

# æ·»åŠ æ–°ä¾èµ–
poetry add requests

# æ·»åŠ å¼€å‘ä¾èµ–
poetry add --group dev pytest

# æ›´æ–°ä¾èµ–
poetry update

# æŸ¥çœ‹ä¾èµ–æ ‘
poetry show --tree
```

### ä½¿ç”¨ Makefile

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
make help

# è®¾ç½®é¡¹ç›®
make setup

# è¿è¡Œæµ‹è¯•
make test

# ä»£ç æ ¼å¼åŒ–
make format

# ä»£ç æ£€æŸ¥
make lint

# æ¸…ç†ç¼“å­˜
make clean
```

### API å¼€å‘

å¦‚æœéœ€è¦å¼€å‘è‡ªå·±çš„ API æœåŠ¡ï¼š

```bash
# å®‰è£… API ä¾èµ–
poetry install --extras "api"

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
make dev-server

# æˆ–æ‰‹åŠ¨å¯åŠ¨
poetry run uvicorn api_server:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“š æ–‡æ¡£èµ„æº

- [ä¸­æ–‡æ–‡æ¡£](docs/zh/README.md) - å®Œæ•´ä¸­æ–‡æ–‡æ¡£
- **[å¿«é€Ÿéƒ¨ç½²æŒ‡å—](docs/zh/quick-start.md)** - å¿«é€Ÿéƒ¨ç½²å’Œæµ‹è¯•æŒ‡å—
- **[Docker Composeå®Œæ•´æŒ‡å—](docs/zh/docker-compose-complete-guide.md)** - ä¸€é”®éƒ¨ç½²Ollama+Nginxå®Œæ•´æœåŠ¡æ ˆ
- **[æ€§èƒ½ä¼˜åŒ–æŒ‡å—](docs/zh/performance-optimization.md)** - è§£å†³å¤§æ¨¡å‹æ¨ç†æ€§èƒ½é—®é¢˜
- **[Hostå®Œæ•´éƒ¨ç½²æŒ‡å—](docs/zh/host-deployment-complete.md)** - ä»è£¸æœºåˆ°HostæœåŠ¡è¿è¡Œçš„å®Œæ•´è¿‡ç¨‹
- **[è£¸æœºå®Œæ•´éƒ¨ç½²æŒ‡å—](docs/zh/bare-metal-deployment.md)** - Complete bare-metal deployment guide
- **[å¿«é€Ÿå®‰è£…å‘½ä»¤å‚è€ƒ](docs/zh/quick-setup-commands.md)** - Quick setup commands reference
- [å®‰è£…æŒ‡å—](docs/zh/installation.md) - Installation guide
- [ç¬¬ä¸‰æ–¹é›†æˆæŒ‡å—](docs/zh/third-party-integration.md) - Third-party integration guide

## ğŸ” æ•…éšœæ’æŸ¥

### æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥ Docker æœåŠ¡
sudo systemctl status docker

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose -f docker-compose-complete.yml ps

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
docker compose -f docker-compose-complete.yml logs ollama

# æ£€æŸ¥ç«¯å£å ç”¨
sudo lsof -i :11434
sudo lsof -i :80

# é‡å¯æœåŠ¡
docker compose -f docker-compose-complete.yml restart
```

### ç«¯å£å†²çª

å¦‚æœ 11434 æˆ– 80 ç«¯å£è¢«å ç”¨ï¼š

```bash
# æŸ¥çœ‹å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo lsof -i :11434
sudo lsof -i :80

# åœæ­¢å†²çªçš„æœåŠ¡
sudo systemctl stop ollama  # å¦‚æœä¸»æœºä¸Šæœ‰ Ollama
sudo systemctl stop nginx   # å¦‚æœä¸»æœºä¸Šæœ‰ Nginx

# æˆ–è€…ä¿®æ”¹ docker-compose-complete.yml ä¸­çš„ç«¯å£æ˜ å°„
# ä¾‹å¦‚å°† 11434 æ”¹ä¸º 11435
```

### æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# è¿›å…¥å®¹å™¨æ‰‹åŠ¨ä¸‹è½½
docker exec -it ollama-service bash
ollama pull qwen2.5:7b

# æˆ–è€…ä»å¤–éƒ¨ä¸‹è½½
docker exec ollama-service ollama pull qwen2.5:7b

# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec ollama-service ping -c 3 ollama.ai

# å¦‚æœç½‘ç»œæœ‰é—®é¢˜ï¼Œå¯ä»¥é…ç½®ä»£ç†
# ç¼–è¾‘ docker-compose-complete.ymlï¼Œæ·»åŠ ç¯å¢ƒå˜é‡ï¼š
# environment:
#   - HTTP_PROXY=http://your-proxy:port
#   - HTTPS_PROXY=http://your-proxy:port
```

### æ€§èƒ½é—®é¢˜

#### å“åº”é€Ÿåº¦æ…¢

```bash
# 1. åˆ‡æ¢åˆ°æ›´å°çš„æ¨¡å‹
# ä» 72B â†’ 32B â†’ 7B

# 2. æ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi

# 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
docker stats ollama-service

# 4. æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker compose -f docker-compose-complete.yml logs ollama | tail -100
```

#### å†…å­˜ä¸è¶³

```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
docker exec ollama-service ollama rm qwen2.5:72b
docker exec ollama-service ollama pull qwen2.5:7b

# 2. è°ƒæ•´ Docker å†…å­˜é™åˆ¶
# ç¼–è¾‘ docker-compose-complete.yml
# å¢åŠ  memory é™åˆ¶

# 3. æ£€æŸ¥ç³»ç»Ÿå†…å­˜
free -h
```

#### GPU æœªè¢«ä½¿ç”¨

```bash
# æ£€æŸ¥ nvidia-docker æ˜¯å¦å®‰è£…
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# æ£€æŸ¥å®¹å™¨æ˜¯å¦èƒ½è®¿é—® GPU
docker exec ollama-service nvidia-smi

# å¦‚æœçœ‹ä¸åˆ° GPUï¼Œé‡æ–°å®‰è£… nvidia-container-toolkit
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
docker compose -f docker-compose-complete.yml restart
```

### API è°ƒç”¨å¤±è´¥

```bash
# 1. æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
docker exec ollama-service ollama list

# 3. æµ‹è¯•ç®€å•è¯·æ±‚
curl http://localhost:11434/api/generate -X POST \
  -H 'Content-Type: application/json' \
  -d '{"model": "qwen2.5:7b", "prompt": "Hi", "stream": false}'

# 4. æ£€æŸ¥è¶…æ—¶è®¾ç½®
# å¤§æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿çš„è¶…æ—¶æ—¶é—´
# åœ¨ Python å®¢æˆ·ç«¯ä¸­å¢åŠ  timeout å‚æ•°
```

### å®¹å™¨å¥åº·æ£€æŸ¥å¤±è´¥

```bash
# æŸ¥çœ‹å¥åº·æ£€æŸ¥æ—¥å¿—
docker inspect ollama-service | grep -A 10 Health

# æ‰‹åŠ¨è¿è¡Œå¥åº·æ£€æŸ¥å‘½ä»¤
docker exec ollama-service ollama list

# å¦‚æœå¤±è´¥ï¼Œé‡å¯å®¹å™¨
docker compose -f docker-compose-complete.yml restart ollama
```

## ğŸŒ å¤–éƒ¨è®¿é—®é…ç½®

### é€šè¿‡ IP è®¿é—®

```bash
# è·å–æœåŠ¡å™¨ IP
SERVER_IP=$(hostname -I | awk '{print $1}')
echo "è®¿é—®åœ°å€: http://$SERVER_IP:11434"

# æµ‹è¯•å¤–éƒ¨è®¿é—®
curl http://$SERVER_IP:11434/api/tags
```

### é€šè¿‡ Nginx è®¿é—®

å½“å‰é…ç½®å·²åŒ…å« Nginx åå‘ä»£ç†ï¼š

```bash
# é€šè¿‡ Nginx è®¿é—®ï¼ˆ80 ç«¯å£ï¼‰
curl http://$SERVER_IP/api/tags

# æŸ¥çœ‹ Nginx é…ç½®
cat nginx/ollama-docker.conf

# æŸ¥çœ‹ Nginx æ—¥å¿—
docker compose -f docker-compose-complete.yml logs nginx
```

### é…ç½®åŸŸåè®¿é—®

å¦‚æœæœ‰åŸŸåï¼Œå¯ä»¥é…ç½® DNS æŒ‡å‘æœåŠ¡å™¨ IPï¼Œç„¶åï¼š

```bash
# ç¼–è¾‘ Nginx é…ç½®
vim nginx/ollama-docker.conf

# æ·»åŠ  server_name
# server_name your-domain.com;

# é‡å¯ Nginx
docker compose -f docker-compose-complete.yml restart nginx
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### é€‰æ‹©åˆé€‚çš„æ¨¡å‹

| åœºæ™¯ | æ¨èæ¨¡å‹ | åŸå›  |
|------|---------|------|
| å¿«é€ŸåŸå‹å¼€å‘ | qwen2.5:7b | å“åº”å¿«ï¼Œèµ„æºå ç”¨å°‘ |
| æ—¥å¸¸ä½¿ç”¨ | qwen2.5:7b | æ€§ä»·æ¯”æœ€é«˜ |
| ä¸“ä¸šåº”ç”¨ | qwen2.5:32b | è´¨é‡å’Œé€Ÿåº¦å¹³è¡¡ |
| é«˜è´¨é‡è¾“å‡º | qwen2.5:72b | æœ€ä½³è´¨é‡ï¼Œéœ€è¦å¼ºå¤§ç¡¬ä»¶ |

### GPU åŠ é€Ÿ

å¦‚æœæœ‰ NVIDIA GPUï¼Œç¡®ä¿æ­£ç¡®é…ç½®ï¼š

```bash
# æ£€æŸ¥ GPU æ˜¯å¦è¢«å®¹å™¨ä½¿ç”¨
docker exec ollama-service nvidia-smi

# æŸ¥çœ‹ GPU åˆ©ç”¨ç‡
watch -n 1 nvidia-smi
```

### å¹¶å‘ä¼˜åŒ–

åœ¨ `docker-compose-complete.yml` ä¸­è°ƒæ•´å¹¶å‘å‚æ•°ï¼š

```yaml
environment:
  - OLLAMA_NUM_PARALLEL=2  # å…è®¸åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°
  - OLLAMA_MAX_LOADED_MODELS=1  # åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°
```

### å†…å­˜ä¼˜åŒ–

```bash
# 1. åªä¿ç•™éœ€è¦çš„æ¨¡å‹
docker exec ollama-service ollama list
docker exec ollama-service ollama rm <ä¸éœ€è¦çš„æ¨¡å‹>

# 2. è°ƒæ•´ Docker å†…å­˜é™åˆ¶
# ç¼–è¾‘ docker-compose-complete.yml ä¸­çš„ memory è®¾ç½®

# 3. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹ï¼ˆé»˜è®¤å·²æ˜¯ Q4_K_M é‡åŒ–ï¼‰
```

## ğŸ“Š API å‚è€ƒ

### Python å®¢æˆ·ç«¯ API

#### OllamaClient

åŒæ­¥å®¢æˆ·ç«¯ï¼Œé€‚åˆç®€å•è„šæœ¬å’Œåº”ç”¨ï¼š

```python
from ollama_client import OllamaClient, OllamaConfig

# åˆ›å»ºå®¢æˆ·ç«¯
client = OllamaClient()

# ä¸»è¦æ–¹æ³•
client.health_check()                    # å¥åº·æ£€æŸ¥
client.list_models()                     # åˆ—å‡ºæ¨¡å‹
client.generate(prompt, **kwargs)        # ç”Ÿæˆæ–‡æœ¬
client.generate_stream(prompt, **kwargs) # æµå¼ç”Ÿæˆ
client.chat(messages, **kwargs)          # èŠå¤©å¯¹è¯
```

#### AsyncOllamaClient

å¼‚æ­¥å®¢æˆ·ç«¯ï¼Œé€‚åˆé«˜å¹¶å‘åº”ç”¨ï¼š

```python
from ollama_client import AsyncOllamaClient

async with AsyncOllamaClient() as client:
    result = await client.generate("Hello")
    async for chunk in client.generate_stream("Hi"):
        print(chunk)
```

### REST API ç«¯ç‚¹

#### ç”Ÿæˆæ–‡æœ¬

```bash
POST http://localhost:11434/api/generate
Content-Type: application/json

{
  "model": "qwen2.5:7b",
  "prompt": "ä½ çš„æç¤ºè¯",
  "stream": false,
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 2048
}
```

#### èŠå¤©å¯¹è¯

```bash
POST http://localhost:11434/api/chat
Content-Type: application/json

{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"}
  ]
}
```

#### åˆ—å‡ºæ¨¡å‹

```bash
GET http://localhost:11434/api/tags
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ’¬ æ”¯æŒä¸åé¦ˆ

é‡åˆ°é—®é¢˜æˆ–æœ‰å»ºè®®ï¼Ÿ

- ğŸ“ [æäº¤ Issue](../../issues)
- ğŸ“– æŸ¥çœ‹[ä¸­æ–‡æ–‡æ¡£](docs/zh/)è·å–è¯¦ç»†æŒ‡å—
- ğŸ’¡ æŸ¥çœ‹æµ‹è¯•è„šæœ¬äº†è§£ä½¿ç”¨ç¤ºä¾‹
- ğŸ” æŸ¥çœ‹æ•…éšœæ’æŸ¥éƒ¨åˆ†è§£å†³å¸¸è§é—®é¢˜

## ğŸ™ è‡´è°¢

- [Ollama](https://ollama.ai/) - ä¼˜ç§€çš„æœ¬åœ° LLM è¿è¡Œæ—¶
- [Qwen](https://github.com/QwenLM/Qwen) - å¼ºå¤§çš„è¯­è¨€æ¨¡å‹
- [Alibaba Cloud](https://www.alibabacloud.com/) - Qwen æ¨¡å‹çš„å¼€å‘è€…
- æ‰€æœ‰è´¡çŒ®è€…å’Œç¤¾åŒºåé¦ˆ

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€

- âœ… Docker Compose éƒ¨ç½²ï¼šç¨³å®šè¿è¡Œ
- âœ… å¤šæ¨¡å‹æ”¯æŒï¼š7B/32B/72B
- âœ… Python å®¢æˆ·ç«¯ï¼šåŒæ­¥/å¼‚æ­¥æ”¯æŒ
- âœ… Nginx åå‘ä»£ç†ï¼šç”Ÿäº§å°±ç»ª
- âœ… GPU åŠ é€Ÿï¼šNVIDIA GPU æ”¯æŒ
- âœ… å¥åº·æ£€æŸ¥ï¼šå®Œæ•´ç›‘æ§
- ğŸ”„ æŒç»­æ›´æ–°ä¸­...

---

**å¿«é€Ÿé“¾æ¥ï¼š**
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [æ¨¡å‹é€‰æ‹©](#-æ¨¡å‹é€‰æ‹©æŒ‡å—)
- [Python ä½¿ç”¨](#-python-å®¢æˆ·ç«¯ä½¿ç”¨)
- [æ•…éšœæ’æŸ¥](#-æ•…éšœæ’æŸ¥)
- [æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–å»ºè®®)