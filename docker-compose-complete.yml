# å®Œæ•´çš„ Ollama + Qwen 72B + Nginx åå‘ä»£ç† Docker Compose é…ç½®
# ä¸€é”®å¯åŠ¨å®Œæ•´çš„AIæœåŠ¡æ ˆ

services:
  # Ollama æœåŠ¡ - ä½¿ç”¨å®˜æ–¹é•œåƒï¼Œæ”¯æŒGPUåŠ é€Ÿ
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    ports:
      - "11434:11434"  # Ollama APIç«¯å£ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      # æ€§èƒ½ä¼˜åŒ–é…ç½®
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_NUM_PARALLEL=1
    volumes:
      # æŒ‚è½½hostä¸Šå·²æœ‰çš„æ¨¡å‹ç›®å½•ï¼ˆé¿å…é‡å¤ä¸‹è½½63GBæ¨¡å‹ï¼‰
      - /usr/share/ollama/.ollama:/root/.ollama
      # æŒ‚è½½è„šæœ¬ç›®å½•
      - ./scripts:/scripts
    restart: unless-stopped
    shm_size: '4gb'  # 7Bæ¨¡å‹éœ€è¦çš„å…±äº«å†…å­˜æ›´å°‘
    # GPUæ”¯æŒé…ç½®
    deploy:
      resources:
        limits:
          memory: 16G  # 7Bæ¨¡å‹å†…å­˜éœ€æ±‚å¤§å¹…é™ä½
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # ä½¿ç”¨hostä¸Šå·²æœ‰çš„æ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      echo 'ğŸš€ å¯åŠ¨ Ollama æœåŠ¡ï¼ˆä½¿ç”¨hostä¸Šå·²æœ‰æ¨¡å‹ï¼‰...'
      ollama serve &
      
      echo 'â³ ç­‰å¾… Ollama æœåŠ¡å¯åŠ¨...'
      sleep 10
      
      echo 'ğŸ“¥ ç¡®ä¿7Bæ¨¡å‹å¯ç”¨...'
      if ! ollama list | grep -q 'qwen2.5:7b'; then
        echo 'æ­£åœ¨ä¸‹è½½ qwen2.5:7b æ¨¡å‹...'
        ollama pull qwen2.5:7b
      else
        echo 'âœ… qwen2.5:7b æ¨¡å‹å·²å­˜åœ¨'
      fi
      
      echo 'ğŸ“‹ æ£€æŸ¥å¯ç”¨æ¨¡å‹...'
      ollama list
      
      echo 'âœ… æœåŠ¡å°±ç»ªï¼ä¸»è¦ä½¿ç”¨7Bæ¨¡å‹ä»¥è·å¾—æ›´å¿«å“åº”é€Ÿåº¦'
      wait
      "
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s   # æ¨¡å‹å·²å­˜åœ¨ï¼Œå¿«é€Ÿå¯åŠ¨

  # Nginx åå‘ä»£ç†æœåŠ¡
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    ports:
      - "80:80"   # HTTPç«¯å£
      - "443:443" # HTTPSç«¯å£ï¼ˆé¢„ç•™ï¼‰
    volumes:
      - ./nginx/ollama-docker.conf:/etc/nginx/conf.d/default.conf:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    environment:
      - SERVER_IP=10.176.202.207  # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  # ollama_data: # ä¸å†éœ€è¦ï¼Œç›´æ¥æŒ‚è½½hostç›®å½•
  #   driver: local
  nginx_logs:
    driver: local

# ç½‘ç»œé…ç½®
networks:
  default:
    name: ollama-network
    driver: bridge
