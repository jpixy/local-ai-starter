# Nginx configuration for Ollama reverse proxy in Docker
# 专用于Docker Compose环境的Ollama反向代理配置

# Upstream for Ollama service (Docker容器间通信)
upstream ollama_backend {
    server ollama:11434;  # 使用Docker Compose服务名
    keepalive 32;
}

# HTTP server block
server {
    listen 80 default_server;
    server_name 10.176.202.207.nip.io localhost _;

    # 日志配置
    access_log /var/log/nginx/ollama_access.log;
    error_log /var/log/nginx/ollama_error.log;

    # 客户端上传限制
    client_max_body_size 100M;
    client_body_timeout 120s;
    client_header_timeout 120s;

    # 健康检查端点
    location /health {
        return 200 'Nginx proxy is working!\nBackend: Ollama Docker service\nTime: $time_local\n';
        add_header Content-Type text/plain;
        access_log off;
    }

    # 根路径重定向到API文档
    location = / {
        return 302 /docs;
    }

    # API文档页面
    location /docs {
        return 200 '<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ollama API - Docker部署</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif; margin: 40px; background: #f5f7fa; }
        .container { max-width: 900px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .status { background: #e8f5e8; color: #2d5f2d; padding: 15px; border-radius: 5px; margin: 20px 0; border-left: 4px solid #28a745; }
        .endpoint { background: #f8f9fa; padding: 15px; margin: 15px 0; border-radius: 5px; border-left: 4px solid #007acc; }
        .method { color: #007acc; font-weight: bold; font-family: monospace; }
        pre { background: #2d3748; color: #e2e8f0; padding: 15px; border-radius: 5px; overflow-x: auto; font-size: 13px; }
        .warning { background: #fff3cd; color: #856404; padding: 10px; border-radius: 5px; margin: 15px 0; border-left: 4px solid #ffc107; }
        h1 { color: #2c3e50; } h2 { color: #34495e; margin-top: 30px; }
    </style>
</head>
<body>
    <div class="container">
        <h1>🦙 Ollama API (Docker Compose 部署)</h1>
        
        <div class="status">
            <strong>✅ 服务状态:</strong> Docker容器化部署<br>
            <strong>🌐 访问地址:</strong> 10.176.202.207.nip.io<br>
            <strong>🐳 后端服务:</strong> ollama:11434 (Docker内部网络)<br>
            <strong>📦 代理服务:</strong> nginx:alpine
        </div>

        <h2>📋 API 端点</h2>
        
        <div class="endpoint">
            <span class="method">GET</span> /api/tags<br>
            <small>获取可用模型列表</small>
        </div>
        
        <div class="endpoint">
            <span class="method">POST</span> /api/generate<br>
            <small>生成文本响应 (支持stream: true/false)</small>
        </div>
        
        <div class="endpoint">
            <span class="method">POST</span> /api/chat<br>
            <small>聊天对话接口</small>
        </div>

        <div class="endpoint">
            <span class="method">GET</span> /health<br>
            <small>Nginx 代理健康检查</small>
        </div>
        
        <h2>🧪 快速测试</h2>

        <h3>1. 检查可用模型</h3>
        <pre>curl http://10.176.202.207.nip.io/api/tags</pre>

        <h3>2. 测试文本生成 (7B模型 - 快速响应)</h3>
        <pre>curl -X POST http://10.176.202.207.nip.io/api/generate -H "Content-Type: application/json" -d "{\"model\": \"qwen2.5:7b\", \"prompt\": \"介绍Docker\", \"stream\": false}"</pre>

        <h3>3. 聊天对话示例</h3>
        <pre>curl -X POST http://10.176.202.207.nip.io/api/chat -H "Content-Type: application/json" -d "{\"model\": \"qwen2.5:7b\", \"messages\": [{\"role\": \"user\", \"content\": \"你好\"}], \"stream\": false}"</pre>

        <div class="warning">
            <strong>⚠️ 注意:</strong> 
            <ul>
                <li>默认使用7B模型，响应速度快（通常5-15秒）</li>
                <li>首次启动会自动下载7B模型（约4GB）</li>
                <li>7B模型内存需求：8-16GB，GPU显存：6-8GB</li>
                <li>仍可使用32B/72B模型，只需在API调用中指定model参数</li>
            </ul>
        </div>

        <h2>🔧 Docker 管理命令</h2>
        <pre># 启动所有服务
docker compose -f docker-compose-complete.yml up -d

# 查看服务状态
docker compose -f docker-compose-complete.yml ps

# 查看实时日志
docker compose -f docker-compose-complete.yml logs -f

# 停止所有服务  
docker compose -f docker-compose-complete.yml down

# 重启服务
docker compose -f docker-compose-complete.yml restart</pre>

        <h2>🔗 访问方式</h2>
        <ul>
            <li><strong>nip.io域名:</strong> http://10.176.202.207.nip.io</li>
            <li><strong>直接IP访问:</strong> http://10.176.202.207</li>
            <li><strong>本地访问:</strong> http://localhost</li>
        </ul>
    </div>
</body>
</html>';
        add_header Content-Type text/html;
    }

    # Ollama API 反向代理
    location /api/ {
        proxy_pass http://ollama_backend;
        
        # 基础代理头
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 大模型推理超时配置
        proxy_connect_timeout 10s;
        proxy_send_timeout 600s;      # 10分钟，适应72B模型
        proxy_read_timeout 600s;      # 10分钟读取超时
        
        # 流式响应支持
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # 支持 Server-Sent Events (SSE)
        proxy_set_header Cache-Control no-cache;
        proxy_set_header X-Accel-Buffering no;
        
        # 错误处理
        proxy_intercept_errors on;
        error_page 502 503 504 = @ollama_error;
    }

    # Ollama 服务错误处理
    location @ollama_error {
        return 503 '{"error": "Ollama service temporarily unavailable. Please check if the ollama container is running."}';
        add_header Content-Type application/json;
    }

    # 管理接口 (可选)
    location /admin/ {
        return 404 '{"error": "Admin interface not configured"}';
        add_header Content-Type application/json;
    }

    # 安全配置
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Server "Ollama-Proxy/1.0";
}
