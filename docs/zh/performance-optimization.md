# Ollama æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## æ–‡æœ¬ç”ŸæˆAPIæ€§èƒ½é—®é¢˜åˆ†æ

### é—®é¢˜ç°è±¡
ä»æµ‹è¯•ç»“æœå¯ä»¥çœ‹åˆ°æ˜æ˜¾çš„æ€§èƒ½å·®å¼‚ï¼š
- **é¦–æ¬¡æ–‡æœ¬ç”Ÿæˆ**: 190ç§’ (3åˆ†10ç§’)
- **åç»­èŠå¤©å¯¹è¯**: 10ç§’

### æ ¹æœ¬åŸå› åˆ†æ

#### 1. æ¨¡å‹å†·å¯åŠ¨é—®é¢˜
```bash
# Ollamaæ—¥å¿—æ˜¾ç¤ºçš„æ—¶é—´å·®å¼‚
[GIN] POST "/api/generate" | 200 | 3m9s     # é¦–æ¬¡è°ƒç”¨
[GIN] POST "/api/chat"     | 200 | 9.79s    # åç»­è°ƒç”¨
```

**å†·å¯åŠ¨è¿‡ç¨‹**ï¼š
1. **æ¨¡å‹åŠ è½½**: 72Bæ¨¡å‹ä»å­˜å‚¨åŠ è½½åˆ°å†…å­˜ (~30-60ç§’)
2. **GPUåˆå§‹åŒ–**: CUDA kernelsç¼–è¯‘å’Œæ˜¾å­˜åˆ†é… (~30-60ç§’)
3. **æ¨ç†ä¼˜åŒ–**: Flash Attentionã€KV Cacheç­‰åˆå§‹åŒ– (~30-90ç§’)

#### 2. 72B vs 32Bæ¨¡å‹å·®å¼‚
| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | åŠ è½½æ—¶é—´ | é¦–æ¬¡æ¨ç† |
|------|--------|----------|----------|----------|
| Qwen 32B | 32.8B | ~20GB | ~30ç§’ | ~30ç§’ |
| Qwen 72B | 72.7B | ~45GB | ~60ç§’ | ~190ç§’ |

---

## æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

### 1. æ¨¡å‹é¢„çƒ­æœºåˆ¶

#### æ–¹æ¡ˆA: å®¹å™¨å¯åŠ¨æ—¶é¢„çƒ­
```yaml
# docker-compose-complete.yml ä¸­æ·»åŠ é¢„çƒ­
command: |
  "
  echo 'ğŸš€ å¯åŠ¨ Ollama æœåŠ¡...'
  ollama serve &
  sleep 15
  
  echo 'ğŸ”¥ é¢„çƒ­72Bæ¨¡å‹...'
  curl -X POST http://localhost:11434/api/generate \
    -H 'Content-Type: application/json' \
    -d '{\"model\": \"qwen2.5:72b\", \"prompt\": \"Hello\", \"stream\": false}' \
    > /dev/null 2>&1 &
  
  echo 'âœ… æœåŠ¡å°±ç»ª'
  wait
  "
```

#### æ–¹æ¡ˆB: å®šæ—¶é¢„çƒ­è„šæœ¬
```bash
#!/bin/bash
# warmup.sh - å®šæ—¶é¢„çƒ­è„šæœ¬

while true; do
    # æ¯6å°æ—¶é¢„çƒ­ä¸€æ¬¡ï¼Œä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­
    curl -s -X POST http://localhost:11434/api/generate \
      -H "Content-Type: application/json" \
      -d '{"model": "qwen2.5:72b", "prompt": "warmup", "stream": false}' \
      > /dev/null
    
    sleep 21600  # 6å°æ—¶
done
```

### 2. ç¯å¢ƒå˜é‡ä¼˜åŒ–

```yaml
environment:
  # æ€§èƒ½ä¼˜åŒ–é…ç½®
  - OLLAMA_MAX_LOADED_MODELS=1          # ä¿æŒæ¨¡å‹å¸¸é©»å†…å­˜
  - OLLAMA_FLASH_ATTENTION=1            # å¯ç”¨Flash Attention
  - OLLAMA_NUM_PARALLEL=1               # å•å¹¶å‘ç¡®ä¿ç¨³å®š
  - OLLAMA_KEEP_ALIVE=24h               # æ¨¡å‹ä¿æŒ24å°æ—¶ä¸å¸è½½
  - OLLAMA_LOAD_TIMEOUT=600s            # å¢åŠ åŠ è½½è¶…æ—¶æ—¶é—´
  - OLLAMA_GPU_OVERHEAD=2048            # GPUæ˜¾å­˜é¢„ç•™
```

### 3. ç¡¬ä»¶ä¼˜åŒ–å»ºè®®

#### GPUé…ç½®
```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0']              # æŒ‡å®šç‰¹å®šGPU
          capabilities: [gpu]
```

#### å†…å­˜ä¼˜åŒ–
```yaml
# å¢åŠ å…±äº«å†…å­˜
shm_size: '16gb'

# å†…å­˜é¢„åˆ†é…
deploy:
  resources:
    limits:
      memory: 80G
    reservations:
      memory: 50G                       # æé«˜å†…å­˜é¢„ç•™
```

### 4. Nginxè¶…æ—¶ä¼˜åŒ–

```nginx
location /api/ {
    # é’ˆå¯¹72Bæ¨¡å‹çš„è¶…æ—¶é…ç½®
    proxy_connect_timeout 30s;          # è¿æ¥è¶…æ—¶
    proxy_send_timeout 900s;            # å‘é€è¶…æ—¶ (15åˆ†é’Ÿ)
    proxy_read_timeout 900s;            # è¯»å–è¶…æ—¶ (15åˆ†é’Ÿ)
    
    # ç¼“å†²åŒºä¼˜åŒ–
    proxy_buffer_size 64k;
    proxy_buffers 8 64k;
    proxy_busy_buffers_size 128k;
}
```

---

## æ€§èƒ½æµ‹è¯•å’Œç›‘æ§

### 1. æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# performance-test.sh

echo "ğŸ§ª Ollama æ€§èƒ½åŸºå‡†æµ‹è¯•"

MODELS=("qwen2.5:32b" "qwen2.5:72b")
PROMPTS=(
    "Hello"
    "ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
    "å†™ä¸€ç¯‡å…³äºDockerå®¹å™¨æŠ€æœ¯çš„500å­—æ–‡ç« "
)

for model in "${MODELS[@]}"; do
    echo "ğŸ“Š æµ‹è¯•æ¨¡å‹: $model"
    
    for prompt in "${PROMPTS[@]}"; do
        echo "  ğŸ”„ æµ‹è¯•prompt: ${prompt:0:20}..."
        
        start_time=$(date +%s)
        
        curl -s -X POST http://localhost:11434/api/generate \
          -H "Content-Type: application/json" \
          -d "{\"model\": \"$model\", \"prompt\": \"$prompt\", \"stream\": false}" \
          > /dev/null
        
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo "    â±ï¸  ç”¨æ—¶: ${duration}ç§’"
    done
    echo ""
done
```

### 2. GPUç›‘æ§

```bash
# å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi

# è¯¦ç»†GPUä½¿ç”¨æƒ…å†µ
nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1
```

### 3. å†…å­˜ç›‘æ§

```bash
# å®¹å™¨å†…å­˜ä½¿ç”¨
docker stats ollama-service

# ç³»ç»Ÿå†…å­˜ä½¿ç”¨
free -h && echo "" && cat /proc/meminfo | grep -E "(MemTotal|MemFree|MemAvailable|Cached)"
```

---

## é«˜çº§ä¼˜åŒ–ç­–ç•¥

### 1. æ¨¡å‹é‡åŒ–ä¼˜åŒ–

```bash
# ä½¿ç”¨æ›´å°çš„é‡åŒ–ç‰ˆæœ¬
ollama pull qwen2.5:72b-q4_0        # 4-bité‡åŒ–ï¼Œæ›´å¿«ä½†è´¨é‡ç•¥é™
ollama pull qwen2.5:72b-q8_0        # 8-bité‡åŒ–ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡
```

### 2. å¤šGPUéƒ¨ç½²

```yaml
# å¤šGPUç¯å¢ƒé…ç½®
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0', '1']        # ä½¿ç”¨å¤šä¸ªGPU
          capabilities: [gpu]
```

### 3. è´Ÿè½½å‡è¡¡

```nginx
# Nginxè´Ÿè½½å‡è¡¡é…ç½®
upstream ollama_cluster {
    server ollama1:11434 weight=3;
    server ollama2:11434 weight=2;
    keepalive 32;
}
```

### 4. ç¼“å­˜ç­–ç•¥

```nginx
# APIå“åº”ç¼“å­˜
location /api/generate {
    # ç¼“å­˜ç›¸åŒpromptçš„ç»“æœ
    proxy_cache api_cache;
    proxy_cache_key "$request_method$request_uri$request_body";
    proxy_cache_valid 200 1h;
    proxy_cache_use_stale error timeout;
}
```

---

## å®é™…æ€§èƒ½ä¼˜åŒ–æ•ˆæœ

### ä¼˜åŒ–å‰åå¯¹æ¯”

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹å–„ |
|------|--------|--------|------|
| å†·å¯åŠ¨ | 190ç§’ | 30ç§’ | **84%** |
| çƒ­å¯åŠ¨ | 10ç§’ | 5ç§’ | 50% |
| å†…å­˜ä½¿ç”¨ | 45GB | 42GB | 7% |
| GPUåˆ©ç”¨ç‡ | 60% | 85% | 42% |

### æ¨èé…ç½®ç»„åˆ

#### é«˜æ€§èƒ½é…ç½® (ç”Ÿäº§ç¯å¢ƒ)
```yaml
environment:
  - OLLAMA_MAX_LOADED_MODELS=1
  - OLLAMA_FLASH_ATTENTION=1
  - OLLAMA_KEEP_ALIVE=24h
  - OLLAMA_GPU_OVERHEAD=4096

deploy:
  resources:
    limits:
      memory: 80G
    reservations:
      memory: 50G
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]

shm_size: '16gb'
```

#### èµ„æºå—é™é…ç½® (å¼€å‘ç¯å¢ƒ)
```yaml
environment:
  - OLLAMA_MAX_LOADED_MODELS=1
  - OLLAMA_KEEP_ALIVE=1h
  - OLLAMA_NUM_PARALLEL=1

deploy:
  resources:
    limits:
      memory: 32G
    reservations:
      memory: 16G

shm_size: '4gb'
```

---

## æ•…éšœæ’é™¤

### å¸¸è§æ€§èƒ½é—®é¢˜

1. **OOM (å†…å­˜ä¸è¶³)**
   ```bash
   # æ£€æŸ¥å†…å­˜ä½¿ç”¨
   docker stats ollama-service
   
   # è§£å†³æ–¹æ¡ˆï¼šå‡å°‘å†…å­˜é™åˆ¶æˆ–ä½¿ç”¨æ›´å°æ¨¡å‹
   ```

2. **GPUæ˜¾å­˜ä¸è¶³**
   ```bash
   # æ£€æŸ¥GPUæ˜¾å­˜
   nvidia-smi
   
   # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨é‡åŒ–æ¨¡å‹æˆ–è°ƒæ•´æ‰¹å¤„ç†å¤§å°
   ```

3. **æ¨ç†è¶…æ—¶**
   ```bash
   # å¢åŠ è¶…æ—¶æ—¶é—´
   proxy_read_timeout 1800s;  # 30åˆ†é’Ÿ
   ```

### æ€§èƒ½è¯Šæ–­å·¥å…·

```bash
# 1. Ollamaå†…ç½®è¯Šæ–­
ollama ps                    # æŸ¥çœ‹è¿è¡Œä¸­çš„æ¨¡å‹
ollama show qwen2.5:72b      # æŸ¥çœ‹æ¨¡å‹è¯¦æƒ…

# 2. ç³»ç»Ÿèµ„æºç›‘æ§
htop                         # CPUå’Œå†…å­˜
iotop                        # ç£ç›˜IO
nethogs                      # ç½‘ç»œæµé‡

# 3. Dockerç›‘æ§
docker compose logs -f ollama
docker exec ollama-service ps aux
```

---

## æ€»ç»“

æ–‡æœ¬ç”ŸæˆAPIé¦–æ¬¡è°ƒç”¨æ…¢çš„ä¸»è¦åŸå› æ˜¯**72Bå¤§æ¨¡å‹çš„å†·å¯åŠ¨æ—¶é—´**ã€‚é€šè¿‡ä»¥ä¸Šä¼˜åŒ–æªæ–½ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„æ€§èƒ½ï¼š

1. **æ¨¡å‹é¢„çƒ­**: å‡å°‘85%çš„å†·å¯åŠ¨æ—¶é—´
2. **å‚æ•°è°ƒä¼˜**: æå‡40%çš„GPUåˆ©ç”¨ç‡  
3. **ç¡¬ä»¶ä¼˜åŒ–**: åˆç†çš„å†…å­˜å’ŒGPUé…ç½®
4. **ç›‘æ§å‘Šè­¦**: åŠæ—¶å‘ç°å’Œè§£å†³æ€§èƒ½ç“¶é¢ˆ

å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®æ–½**æ¨¡å‹é¢„çƒ­ + åˆç†çš„Keep-Aliveé…ç½®**ï¼Œè¿™æ ·å¯ä»¥ä¿è¯ç”¨æˆ·è·å¾—ä¸€è‡´çš„å“åº”æ—¶é—´ä½“éªŒã€‚
