# Nginx configuration for Ollama reverse proxy in Docker
# 专用于Docker Compose环境的Ollama反向代理配置

# Upstream for Ollama service (Docker容器间通信)
upstream ollama_backend {
    server ollama:11434;  # 使用Docker Compose服务名
    keepalive 32;
}

# HTTP server block
server {
    listen 80 default_server;
    server_name 10.176.202.207.nip.io localhost _;

    # 日志配置
    access_log /var/log/nginx/ollama_access.log;
    error_log /var/log/nginx/ollama_error.log;

    # 客户端上传限制
    client_max_body_size 100M;
    client_body_timeout 120s;
    client_header_timeout 120s;

    # 健康检查端点
    location /health {
        return 200 'Nginx proxy is working!\nBackend: Ollama Docker service\nTime: $time_local\n';
        add_header Content-Type text/plain;
        access_log off;
    }

    # 根路径重定向到API文档
    location = / {
        return 302 /docs;
    }

    # API文档页面 - 使用简化版本避免Nginx多行问题
    location /docs {
        return 200 '<html><head><title>Ollama API - 7B Model</title><style>body{font-family:Arial,sans-serif;margin:40px;background:#f5f7fa}.container{max-width:800px;margin:0 auto;background:white;padding:30px;border-radius:8px}.status{background:#e8f5e8;color:#2d5f2d;padding:15px;border-radius:5px;margin:20px 0}.endpoint{background:#f8f9fa;padding:15px;margin:15px 0;border-radius:5px}pre{background:#2d3748;color:#e2e8f0;padding:15px;border-radius:5px;overflow-x:auto}</style></head><body><div class="container"><h1>🦙 Ollama API (7B快速模型)</h1><div class="status"><strong>✅ 状态:</strong> Docker部署运行中<br><strong>🌐 地址:</strong> 10.176.202.207.nip.io<br><strong>🚀 模型:</strong> qwen2.5:7b (快速响应)</div><h2>📋 API端点</h2><div class="endpoint"><strong>GET</strong> /api/tags - 模型列表</div><div class="endpoint"><strong>POST</strong> /api/generate - 文本生成</div><div class="endpoint"><strong>POST</strong> /api/chat - 聊天对话</div><h2>🧪 测试示例</h2><pre>curl http://10.176.202.207.nip.io/api/tags</pre><pre>curl -X POST http://10.176.202.207.nip.io/api/generate -H "Content-Type: application/json" -d "{\"model\": \"qwen2.5:7b\", \"prompt\": \"Hello\", \"stream\": false}"</pre><p>💡 7B模型响应速度：5-15秒<br>🔄 可选用32B/72B模型，修改model参数即可</p></div></body></html>';
        add_header Content-Type text/html;
    }

    # Ollama API 反向代理
    location /api/ {
        proxy_pass http://ollama_backend;
        
        # 基础代理头
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 大模型推理超时配置
        proxy_connect_timeout 10s;
        proxy_send_timeout 600s;      # 10分钟，适应72B模型
        proxy_read_timeout 600s;      # 10分钟读取超时
        
        # 流式响应支持
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # 支持 Server-Sent Events (SSE)
        proxy_set_header Cache-Control no-cache;
        proxy_set_header X-Accel-Buffering no;
        
        # 错误处理
        proxy_intercept_errors on;
        error_page 502 503 504 = @ollama_error;
    }

    # Ollama 服务错误处理
    location @ollama_error {
        return 503 '{"error": "Ollama service temporarily unavailable. Please check if the ollama container is running."}';
        add_header Content-Type application/json;
    }

    # 管理接口 (可选)
    location /admin/ {
        return 404 '{"error": "Admin interface not configured"}';
        add_header Content-Type application/json;
    }

    # 安全配置
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Server "Ollama-Proxy/1.0";
}
