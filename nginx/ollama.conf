# Nginx configuration for Ollama reverse proxy
# 配置文件：/etc/nginx/conf.d/ollama.conf

# Upstream for Ollama service
upstream ollama_backend {
    server 127.0.0.1:11434;
    keepalive 32;
}

# HTTP server block
server {
    listen 80;
    server_name 10.176.202.207.nip.io;

    # 日志配置
    access_log /var/log/nginx/ollama_access.log;
    error_log /var/log/nginx/ollama_error.log;

    # 客户端上传限制
    client_max_body_size 100M;
    client_body_timeout 120s;
    client_header_timeout 120s;

    # 健康检查端点
    location /health {
        return 200 'Nginx proxy is working!\n';
        add_header Content-Type text/plain;
        access_log off;
    }

    # 根路径重定向到健康检查
    location = / {
        return 302 /health;
    }

    # API文档页面 - 使用简化的HTML
    location /docs {
        return 200 '<html><head><title>Ollama API</title></head><body><h1>Ollama API via Nginx</h1><p>API Endpoints:</p><ul><li>GET /api/tags - List models</li><li>POST /api/generate - Generate text</li><li>POST /api/chat - Chat interface</li><li>GET /health - Health check</li></ul><p>Test: <code>curl http://10.176.202.207.nip.io/api/tags</code></p></body></html>';
        add_header Content-Type text/html;
    }

    # Ollama API 反向代理
    location /api/ {
        proxy_pass http://ollama_backend;
        
        # 基础代理头
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 大模型推理超时配置
        proxy_connect_timeout 10s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        # 流式响应支持
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # 支持 Server-Sent Events (SSE)
        proxy_set_header Cache-Control no-cache;
        proxy_set_header X-Accel-Buffering no;
        
        # 错误处理
        proxy_intercept_errors on;
        error_page 502 503 504 = @ollama_error;
    }

    # Ollama 服务错误处理
    location @ollama_error {
        return 503 '{"error": "Ollama service temporarily unavailable. Please check if the service is running on port 11434."}';
        add_header Content-Type application/json;
    }

    # 安全配置
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
}
