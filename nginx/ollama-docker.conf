# Nginx configuration for Ollama reverse proxy in Docker
# ä¸“ç”¨äºDocker Composeç¯å¢ƒçš„Ollamaåå‘ä»£ç†é…ç½®

# Upstream for Ollama service (Dockerå®¹å™¨é—´é€šä¿¡)
upstream ollama_backend {
    server ollama:11434;  # ä½¿ç”¨Docker ComposeæœåŠ¡å
    keepalive 32;
}

# HTTP server block
server {
    listen 80 default_server;
    server_name 10.176.202.207.nip.io localhost _;

    # æ—¥å¿—é…ç½®
    access_log /var/log/nginx/ollama_access.log;
    error_log /var/log/nginx/ollama_error.log;

    # å®¢æˆ·ç«¯ä¸Šä¼ é™åˆ¶
    client_max_body_size 100M;
    client_body_timeout 120s;
    client_header_timeout 120s;

    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /health {
        return 200 'Nginx proxy is working!\nBackend: Ollama Docker service\nTime: $time_local\n';
        add_header Content-Type text/plain;
        access_log off;
    }

    # æ ¹è·¯å¾„é‡å®šå‘åˆ°APIæ–‡æ¡£
    location = / {
        return 302 /docs;
    }

    # APIæ–‡æ¡£é¡µé¢ - ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬é¿å…Nginxå¤šè¡Œé—®é¢˜
    location /docs {
        return 200 '<html><head><title>Ollama API - 7B Model</title><style>body{font-family:Arial,sans-serif;margin:40px;background:#f5f7fa}.container{max-width:800px;margin:0 auto;background:white;padding:30px;border-radius:8px}.status{background:#e8f5e8;color:#2d5f2d;padding:15px;border-radius:5px;margin:20px 0}.endpoint{background:#f8f9fa;padding:15px;margin:15px 0;border-radius:5px}pre{background:#2d3748;color:#e2e8f0;padding:15px;border-radius:5px;overflow-x:auto}</style></head><body><div class="container"><h1>ğŸ¦™ Ollama API (7Bå¿«é€Ÿæ¨¡å‹)</h1><div class="status"><strong>âœ… çŠ¶æ€:</strong> Dockeréƒ¨ç½²è¿è¡Œä¸­<br><strong>ğŸŒ åœ°å€:</strong> 10.176.202.207.nip.io<br><strong>ğŸš€ æ¨¡å‹:</strong> qwen2.5:7b (å¿«é€Ÿå“åº”)</div><h2>ğŸ“‹ APIç«¯ç‚¹</h2><div class="endpoint"><strong>GET</strong> /api/tags - æ¨¡å‹åˆ—è¡¨</div><div class="endpoint"><strong>POST</strong> /api/generate - æ–‡æœ¬ç”Ÿæˆ</div><div class="endpoint"><strong>POST</strong> /api/chat - èŠå¤©å¯¹è¯</div><h2>ğŸ§ª æµ‹è¯•ç¤ºä¾‹</h2><pre>curl http://10.176.202.207.nip.io/api/tags</pre><pre>curl -X POST http://10.176.202.207.nip.io/api/generate -H "Content-Type: application/json" -d "{\"model\": \"qwen2.5:7b\", \"prompt\": \"Hello\", \"stream\": false}"</pre><p>ğŸ’¡ 7Bæ¨¡å‹å“åº”é€Ÿåº¦ï¼š5-15ç§’<br>ğŸ”„ å¯é€‰ç”¨32B/72Bæ¨¡å‹ï¼Œä¿®æ”¹modelå‚æ•°å³å¯</p></div></body></html>';
        add_header Content-Type text/html;
    }

    # Ollama API åå‘ä»£ç†
    location /api/ {
        proxy_pass http://ollama_backend;
        
        # åŸºç¡€ä»£ç†å¤´
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # å¤§æ¨¡å‹æ¨ç†è¶…æ—¶é…ç½®
        proxy_connect_timeout 10s;
        proxy_send_timeout 600s;      # 10åˆ†é’Ÿï¼Œé€‚åº”72Bæ¨¡å‹
        proxy_read_timeout 600s;      # 10åˆ†é’Ÿè¯»å–è¶…æ—¶
        
        # æµå¼å“åº”æ”¯æŒ
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # æ”¯æŒ Server-Sent Events (SSE)
        proxy_set_header Cache-Control no-cache;
        proxy_set_header X-Accel-Buffering no;
        
        # é”™è¯¯å¤„ç†
        proxy_intercept_errors on;
        error_page 502 503 504 = @ollama_error;
    }

    # Ollama æœåŠ¡é”™è¯¯å¤„ç†
    location @ollama_error {
        return 503 '{"error": "Ollama service temporarily unavailable. Please check if the ollama container is running."}';
        add_header Content-Type application/json;
    }

    # ç®¡ç†æ¥å£ (å¯é€‰)
    location /admin/ {
        return 404 '{"error": "Admin interface not configured"}';
        add_header Content-Type application/json;
    }

    # å®‰å…¨é…ç½®
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Server "Ollama-Proxy/1.0";
}
